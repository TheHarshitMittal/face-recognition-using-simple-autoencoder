{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133d62da-d2aa-4084-b86e-d06a40efb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c51a3c7-1667-4766-bda7-63d62df77c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 images in the dataset\n",
      "unique target number: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n",
      "X_train shape: (280, 4096)\n",
      "y_train shape:(280,)\n"
     ]
    }
   ],
   "source": [
    "image_data = fetch_olivetti_faces()\n",
    "X, target = image_data.data, image_data.target\n",
    "print(\"There are {} images in the dataset\".format(len(X)))\n",
    "print(\"unique target number:\",np.unique(target))\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.3, stratify=target, random_state=0)\n",
    "print(\"X_train shape:\",X_train.shape)\n",
    "print(\"y_train shape:{}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "249911c1-7936-4da5-8127-e5ef5b02ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras import Input, Model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6df513-e11c-4667-862b-178c3ea6ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 15 \n",
    "input_img = Input(shape=(4096,))\n",
    "# encoded representation of input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# decoded representation of code \n",
    "decoded = Dense(4096, activation='sigmoid')(encoded)\n",
    "# Model which take input image and shows decoded images\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6869906-2245-47ea-ba58-8b44717dd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model shows encoded images\n",
    "encoder = Model(input_img, encoded)\n",
    "# Creating a decoder model\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c292d3-d624-4de0-a692-d7d1d557a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7161da1e-8475-4453-8e9b-dab8dbf97e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2/2 [==============================] - 1s 312ms/step - loss: 0.6937 - val_loss: 0.6929\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6929 - val_loss: 0.6924\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6924 - val_loss: 0.6916\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6916 - val_loss: 0.6907\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6908 - val_loss: 0.6895\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6896 - val_loss: 0.6883\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6884 - val_loss: 0.6867\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6869 - val_loss: 0.6851\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6854 - val_loss: 0.6833\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6837 - val_loss: 0.6816\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6820 - val_loss: 0.6798\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6803 - val_loss: 0.6780\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6785 - val_loss: 0.6763\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6769 - val_loss: 0.6746\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6753 - val_loss: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba84e83dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=15,\n",
    "                batch_size=256,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b247fe-d70a-4753-af76-e19f83be6d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:0.88333\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"accuracy score:{:.5f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc444b3-dd86-4a6d-b242-dc6f3abf38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:0.94167\n"
     ]
    }
   ],
   "source": [
    "lr=LinearDiscriminantAnalysis()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "print(\"Accuracy score:{:.5f}\".format(metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22915c2d-12a9-467d-b57b-174379f3bd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
